{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Entity Detection in Text",
      "provenance": [],
      "authorship_tag": "ABX9TyNGyZdgEyydSZWuhoevYGc5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhimanyu1214/Entity-Detection-in-Text/blob/main/Entity_Detection_in_Text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6n1_klE2h1T"
      },
      "source": [
        "Before developing and training your own NER model, it's worth your time to first consider the requirements of your project and try out some of the preexisting off-the-shelf NER models to see if they can do the job for you. Preexisting NER models have the advantage of being ready to test in a few lines of code and are in some cases designed around being fast and robust in a production setting.\r\n",
        "\r\n",
        "If your project requires you to identify basic NER types like people, organizations, locations, etc. then I encourage you to first test your project with the existing NER models from spaCy, Stanford, and Flair.\r\n",
        "\r\n",
        "The following code cell shows how to retrieve entity tags for some text using spaCy, which comes pre-installed on Colab.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UruskRWBtEB9",
        "outputId": "2be7f416-5729-4c7c-9520-1d45cb4f5b4c"
      },
      "source": [
        "import spacy\r\n",
        "\r\n",
        "# Download a spacy model for processing English\r\n",
        "nlp = spacy.load(\"en_core_web_sm\")\r\n",
        "\r\n",
        "# Process a sentence using the spacy model\r\n",
        "doc = nlp(\"Google LLC is an American multinational technology company that specializes in Internet-related services and products, which include online advertising technologies, a search engine, cloud computing, software, and hardware. It is considered one of the Big Five technology companies in the U.S. information technology industry, alongside Amazon, Facebook, Apple, and Microsoft.\")\r\n",
        "doc = nlp(\"Google was founded in September 1998 by Larry Page and Sergey Brin while they were Ph.D. students at Stanford University in California. Together they own about 14 percent of its shares and control 56 percent of the stockholder voting power through supervoting stock. They incorporated Google as a California privately held company on September 4, 1998. Google was then reincorporated in Delaware on October 22, 2002.\")\r\n",
        "# Display the entities found by the model, and the type of each.\r\n",
        "print('{:<12}  {:}\\n'.format('Entity', 'Type'))\r\n",
        "\r\n",
        "# For each entity found...\r\n",
        "for ent in doc.ents:\r\n",
        "    \r\n",
        "    # Print the entity text `ent.text` and its label `ent.label_`.\r\n",
        "    print('{:<12}  {:}'.format(ent.text, ent.label_))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Entity        Type\n",
            "\n",
            "Google        ORG\n",
            "September 1998  DATE\n",
            "Larry Page    PERSON\n",
            "Sergey Brin   PERSON\n",
            "Ph.D.         WORK_OF_ART\n",
            "Stanford University  ORG\n",
            "California    GPE\n",
            "about 14 percent  PERCENT\n",
            "56 percent    PERCENT\n",
            "Google        ORG\n",
            "California    GPE\n",
            "September 4, 1998  DATE\n",
            "Google        ORG\n",
            "Delaware      GPE\n",
            "October 22, 2002  DATE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "doEx7Tlx2AT2"
      },
      "source": [
        "Named Entity Recognition (NER) tasks here, we wanted to provide some practical guidance and resources for building your own NER application since fine-tuning BERT may not be the best solution for every NER application.\r\n",
        "\r\n",
        "In this post, we will:\r\n",
        "\r\n",
        "1.Discuss when it might be appropriate to use an off-the-shelf library vs training / fine-tuning your own model.\r\n",
        "2.Point you to some popular libraries for performing NER tagging and share some quick-start examples.\r\n",
        "3.Share some resources we've found comparing and benchmarking different NER tools.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ornu4xGE2wpZ"
      },
      "source": [
        "When to Fine-Tune\r\n",
        "\r\n",
        "In some cases, these off-the-shelf libraries won't be the best solution for your project. You might have:\r\n",
        "\r\n",
        "    Specific entity types that are not included in the off-the-shelf versions\r\n",
        "    A different kind of text corpus from what the off-the-shelf models are trained on\r\n",
        "    Very high accuracy or recall requirements\r\n",
        "\r\n",
        "In general, fine-tuning BERT (or variants of BERT) on your dataset will yield a highly accurate tagger, and with less training data required than training a custom model from scratch.\r\n",
        "\r\n",
        "The biggest caveat, however, is that BERT models are large, and typically warrant GPU acceleration. Working with GPUs can be expensive, and BERT will be slower to run on text than tools like spaCy.\r\n",
        "\r\n",
        "So consider your production requirements for speed, accuracy, and cost before going straight to BERT!\r\n",
        "Resources & Experiments\r\n",
        "\r\n",
        "There is, of course, a lot more that can be said about these different NLP toolkits. Our goal for this post was to simply make sure that you were aware of them, and the reasons you might use them over BERT.\r\n",
        "\r\n",
        "The following are some articles which we found informative, containing experiments, summaries, benchmarks, and other comparisons of different NER tools. They're worth looking through if you'd like to get a sense of NER pipelines and the power of existing NER tools. Below each article, we've highlighted the main points."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Kf8e637uYBG",
        "outputId": "e3330d5d-98d0-447d-f23c-5fd53f4fa1c9"
      },
      "source": [
        "from flair.data import Sentence\r\n",
        "from flair.models import SequenceTagger\r\n",
        "\r\n",
        "# Make a sentence\r\n",
        "sentence = Sentence(\"The company's rapid growth since incorporation has triggered a chain of products, acquisitions, and partnerships beyond Google's core search engine (Google Search). It offers services designed for work and productivity (Google Docs, Google Sheets, and Google Slides), email (Gmail), scheduling and time management (Google Calendar), cloud storage (Google Drive), instant messaging and video chat (Duo, Hangouts, Chat, and Meet), language translation (Google Translate), mapping and navigation (Google Maps, Waze, Google Earth, and Street View), podcast hosting (Google Podcasts), video sharing (YouTube), blog publishing (Blogger), note-taking (Google Keep and Google Jamboard), and photo organizing and editing (Google Photos). \")\r\n",
        "\r\n",
        "# Load the NER tagger\r\n",
        "# This file is around 1.5 GB so will take a little while to load.\r\n",
        "tagger = SequenceTagger.load('ner-ontonotes')\r\n",
        "\r\n",
        "# Run NER over sentence\r\n",
        "tagger.predict(sentence)\r\n",
        "\r\n",
        "# Retrieve the entities found by the tagger.\r\n",
        "entity_dict = sentence.to_dict(tag_type='ner')\r\n",
        "\r\n",
        "# Display the entities, and the type(s) of each.\r\n",
        "print('\\n{:<12}  {:}\\n'.format('Entity', 'Type(s)'))\r\n",
        "\r\n",
        "# For each entity...\r\n",
        "for entity in entity_dict['entities']:\r\n",
        "    \r\n",
        "    # Print the entity text and its labels. Flair supports multiple labels\r\n",
        "    # per entity, and includes a confidence score.\r\n",
        "    print('{:<12}  {:}'.format(entity[\"text\"], str(entity[\"labels\"])))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-01-28 02:28:50,436 loading file /root/.flair/models/en-ner-ontonotes-v0.4.pt\n",
            "\n",
            "Entity        Type(s)\n",
            "\n",
            "Google        [ORG (0.9999)]\n",
            "Google Search  [ORG (0.6656)]\n",
            "Google Docs   [ORG (0.7732)]\n",
            "Google Sheets  [ORG (0.4692)]\n",
            "Google Slides  [ORG (0.6962)]\n",
            "Google Calendar  [ORG (0.4058)]\n",
            "Google Drive  [PRODUCT (0.725)]\n",
            "Duo           [PRODUCT (0.8896)]\n",
            "Hangouts      [PRODUCT (0.6412)]\n",
            "Meet          [ORG (0.9559)]\n",
            "Google        [ORG (0.6744)]\n",
            "Translate     [PRODUCT (0.5829)]\n",
            "Google Maps   [PRODUCT (0.7484)]\n",
            "Waze          [ORG (0.8816)]\n",
            "Google Earth  [ORG (0.6109)]\n",
            "Street View   [ORG (0.7651)]\n",
            "Google Podcasts  [ORG (0.618)]\n",
            "YouTube       [ORG (0.8468)]\n",
            "Blogger       [ORG (0.7347)]\n",
            "Google        [ORG (0.9501)]\n",
            "Jamboard      [PRODUCT (0.8827)]\n",
            "Google Photos  [ORG (0.641)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_IsBeq82bXy"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45wpDQJ72_tC"
      },
      "source": [
        "We'll first need to install the library from GitHub. Then we have to install it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9thzkWyaull_",
        "outputId": "a77990d7-6000-4267-f976-18d87c5af10e"
      },
      "source": [
        "!pip install --upgrade git+https://github.com/flairNLP/flair.git"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/flairNLP/flair.git\n",
            "  Cloning https://github.com/flairNLP/flair.git to /tmp/pip-req-build-hq4t_nkq\n",
            "  Running command git clone -q https://github.com/flairNLP/flair.git /tmp/pip-req-build-hq4t_nkq\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from flair==0.7) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: matplotlib>=2.2.3 in /usr/local/lib/python3.6/dist-packages (from flair==0.7) (3.2.2)\n",
            "Requirement already satisfied, skipping upgrade: numpy<1.20.0 in /usr/local/lib/python3.6/dist-packages (from flair==0.7) (1.19.5)\n",
            "Collecting ftfy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ff/e2/3b51c53dffb1e52d9210ebc01f1fb9f2f6eba9b3201fa971fd3946643c71/ftfy-5.8.tar.gz (64kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 4.4MB/s \n",
            "\u001b[?25hCollecting huggingface-hub\n",
            "  Downloading https://files.pythonhosted.org/packages/b6/81/522aaa0e08224477c7fad38546003b2b83ee7f76e4b6150977fb5d595276/huggingface_hub-0.0.1-py3-none-any.whl\n",
            "Collecting sqlitedict>=1.6.0\n",
            "  Downloading https://files.pythonhosted.org/packages/5c/2d/b1d99e9ad157dd7de9cd0d36a8a5876b13b55e4b75f7498bc96035fb4e96/sqlitedict-1.7.0.tar.gz\n",
            "Collecting sentencepiece<=0.1.91\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 19.8MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: hyperopt>=0.1.1 in /usr/local/lib/python3.6/dist-packages (from flair==0.7) (0.1.2)\n",
            "Requirement already satisfied, skipping upgrade: tqdm>=4.26.0 in /usr/local/lib/python3.6/dist-packages (from flair==0.7) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: torch>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from flair==0.7) (1.7.0+cu101)\n",
            "Requirement already satisfied, skipping upgrade: lxml in /usr/local/lib/python3.6/dist-packages (from flair==0.7) (4.2.6)\n",
            "Requirement already satisfied, skipping upgrade: tabulate in /usr/local/lib/python3.6/dist-packages (from flair==0.7) (0.8.7)\n",
            "Collecting deprecated>=1.2.4\n",
            "  Downloading https://files.pythonhosted.org/packages/d4/56/7d4774533d2c119e1873993d34d313c9c9efc88c5e4ab7e33bdf915ad98c/Deprecated-1.2.11-py2.py3-none-any.whl\n",
            "Collecting konoha<5.0.0,>=4.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/ea/01/47358efec5396fc80f98273c42cbdfe7aab056252b07884ffcc0f118978f/konoha-4.6.2-py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: regex in /usr/local/lib/python3.6/dist-packages (from flair==0.7) (2019.12.20)\n",
            "Requirement already satisfied, skipping upgrade: scikit-learn>=0.21.3 in /usr/local/lib/python3.6/dist-packages (from flair==0.7) (0.22.2.post1)\n",
            "Collecting langdetect\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/56/a3/8407c1e62d5980188b4acc45ef3d94b933d14a2ebc9ef3505f22cf772570/langdetect-1.0.8.tar.gz (981kB)\n",
            "\u001b[K     |████████████████████████████████| 983kB 47.6MB/s \n",
            "\u001b[?25hCollecting transformers>=4.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/88/b1/41130a228dd656a1a31ba281598a968320283f48d42782845f6ba567f00b/transformers-4.2.2-py3-none-any.whl (1.8MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8MB 46.4MB/s \n",
            "\u001b[?25hCollecting bpemb>=0.3.2\n",
            "  Downloading https://files.pythonhosted.org/packages/91/77/3f0f53856e86af32b1d3c86652815277f7b5f880002584eb30db115b6df5/bpemb-0.3.2-py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: gensim<=3.8.3,>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from flair==0.7) (3.6.0)\n",
            "Collecting segtok>=1.5.7\n",
            "  Downloading https://files.pythonhosted.org/packages/41/08/582dab5f4b1d5ca23bc6927b4bb977c8ff7f3a87a3b98844ef833e2f5623/segtok-1.5.10.tar.gz\n",
            "Collecting mpld3==0.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/95/a52d3a83d0a29ba0d6898f6727e9858fe7a43f6c2ce81a5fe7e05f0f4912/mpld3-0.3.tar.gz (788kB)\n",
            "\u001b[K     |████████████████████████████████| 798kB 46.4MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: gdown in /usr/local/lib/python3.6/dist-packages (from flair==0.7) (3.6.4)\n",
            "Collecting janome\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/63/98858cbead27df7536c7e300c169da0999e9704d02220dc6700b804eeff0/Janome-0.4.1-py2.py3-none-any.whl (19.7MB)\n",
            "\u001b[K     |████████████████████████████████| 19.7MB 1.2MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->flair==0.7) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair==0.7) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair==0.7) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->flair==0.7) (1.3.1)\n",
            "Requirement already satisfied, skipping upgrade: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy->flair==0.7) (0.2.5)\n",
            "Requirement already satisfied, skipping upgrade: filelock in /usr/local/lib/python3.6/dist-packages (from huggingface-hub->flair==0.7) (3.0.12)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from huggingface-hub->flair==0.7) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair==0.7) (0.16.0)\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair==0.7) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: networkx in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair==0.7) (2.5)\n",
            "Requirement already satisfied, skipping upgrade: pymongo in /usr/local/lib/python3.6/dist-packages (from hyperopt>=0.1.1->flair==0.7) (3.11.2)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.5.0->flair==0.7) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.5.0->flair==0.7) (0.8)\n",
            "Requirement already satisfied, skipping upgrade: wrapt<2,>=1.10 in /usr/local/lib/python3.6/dist-packages (from deprecated>=1.2.4->flair==0.7) (1.12.1)\n",
            "Collecting overrides==3.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/42/8d/caa729f809ecdf8e76fac3c1ff7d3f0b72c398c9dd8a6919927a30a873b3/overrides-3.0.0.tar.gz\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.21.3->flair==0.7) (1.0.0)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers>=4.0.0->flair==0.7) (3.4.0)\n",
            "Requirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.6/dist-packages (from transformers>=4.0.0->flair==0.7) (20.8)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 41.6MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.9.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 34.3MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim<=3.8.3,>=3.4.0->flair==0.7) (4.1.2)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->huggingface-hub->flair==0.7) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->huggingface-hub->flair==0.7) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->huggingface-hub->flair==0.7) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->huggingface-hub->flair==0.7) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx->hyperopt>=0.1.1->flair==0.7) (4.4.2)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers>=4.0.0->flair==0.7) (3.4.0)\n",
            "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=4.0.0->flair==0.7) (7.1.2)\n",
            "Building wheels for collected packages: flair\n",
            "  Building wheel for flair (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for flair: filename=flair-0.7-cp36-none-any.whl size=227515 sha256=667221cc32a831277e32cc9def1c7d7b5fb16f62d3aec5d6825cca47fb5b78c9\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-gcglb4zu/wheels/84/82/73/d2b3b59b7be74ea05f2c6d64132efe27df52daffb47d1dc7bb\n",
            "Successfully built flair\n",
            "Building wheels for collected packages: ftfy, sqlitedict, langdetect, segtok, mpld3, overrides, sacremoses\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-5.8-cp36-none-any.whl size=45613 sha256=8cc6aaaa51d91f0798945441b30340aefddc8e578108e4203ad2c60d00055c91\n",
            "  Stored in directory: /root/.cache/pip/wheels/ba/c0/ef/f28c4da5ac84a4e06ac256ca9182fc34fa57fefffdbc68425b\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlitedict: filename=sqlitedict-1.7.0-cp36-none-any.whl size=14376 sha256=d2429589f4e2871fe2c1f7ca38f81fa57ed894da3c90f5f5e6370f9eeae7d5b0\n",
            "  Stored in directory: /root/.cache/pip/wheels/cf/c6/4f/2c64a43f041415eb8b8740bd80e15e92f0d46c5e464d8e4b9b\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.8-cp36-none-any.whl size=993194 sha256=49f47149aaae45c54417362e112b0d480646ffc0e422d9ec3708c8df8030b931\n",
            "  Stored in directory: /root/.cache/pip/wheels/8d/b3/aa/6d99de9f3841d7d3d40a60ea06e6d669e8e5012e6c8b947a57\n",
            "  Building wheel for segtok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for segtok: filename=segtok-1.5.10-cp36-none-any.whl size=25019 sha256=e07f7437fbd4ca0487a86038be3a830519f076dfd9c2d96499436c97db07bf9d\n",
            "  Stored in directory: /root/.cache/pip/wheels/b4/39/f6/9ca1c5cabde964d728023b5751c3a206a5c8cc40252321fb6b\n",
            "  Building wheel for mpld3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpld3: filename=mpld3-0.3-cp36-none-any.whl size=116678 sha256=c3dc0311d167e929d02af95c69e1c4653153360d0573838339523d225842ba1a\n",
            "  Stored in directory: /root/.cache/pip/wheels/c0/47/fb/8a64f89aecfe0059830479308ad42d62e898a3e3cefdf6ba28\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for overrides: filename=overrides-3.0.0-cp36-none-any.whl size=5669 sha256=cbeca4b205e509558249bb36bf2bd5def0db7c61cacb3c6d154d5f235934386c\n",
            "  Stored in directory: /root/.cache/pip/wheels/6f/1b/ec/6c71a1eb823df7f850d956b2d8c50a6d49c191e1063d73b9be\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=c2a26b69e72ce4cdf72d7fc40686e3fa8383a7bbe843773f5ca582b6ef00dae9\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built ftfy sqlitedict langdetect segtok mpld3 overrides sacremoses\n",
            "Installing collected packages: ftfy, huggingface-hub, sqlitedict, sentencepiece, deprecated, overrides, konoha, langdetect, sacremoses, tokenizers, transformers, bpemb, segtok, mpld3, janome, flair\n",
            "Successfully installed bpemb-0.3.2 deprecated-1.2.11 flair-0.7 ftfy-5.8 huggingface-hub-0.0.1 janome-0.4.1 konoha-4.6.2 langdetect-1.0.8 mpld3-0.3 overrides-3.0.0 sacremoses-0.0.43 segtok-1.5.10 sentencepiece-0.1.91 sqlitedict-1.7.0 tokenizers-0.9.4 transformers-4.2.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3HeJNiZ3GjU"
      },
      "source": [
        "install stanza"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "waQgyFWNvPOS",
        "outputId": "da18c187-5854-48f9-974c-89697c1aa4a2"
      },
      "source": [
        "!pip install stanza"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting stanza\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/ae/a70a58ce6b4e2daad538688806ee0f238dbe601954582a74ea57cde6c532/stanza-1.2-py3-none-any.whl (282kB)\n",
            "\r\u001b[K     |█▏                              | 10kB 12.5MB/s eta 0:00:01\r\u001b[K     |██▎                             | 20kB 17.7MB/s eta 0:00:01\r\u001b[K     |███▌                            | 30kB 13.3MB/s eta 0:00:01\r\u001b[K     |████▋                           | 40kB 13.0MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 51kB 14.6MB/s eta 0:00:01\r\u001b[K     |███████                         | 61kB 13.5MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 71kB 10.9MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 81kB 11.7MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 92kB 10.5MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 102kB 10.6MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 112kB 10.6MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 122kB 10.6MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 133kB 10.6MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 143kB 10.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 153kB 10.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 163kB 10.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 174kB 10.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 184kB 10.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 194kB 10.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 204kB 10.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 215kB 10.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 225kB 10.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 235kB 10.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 245kB 10.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 256kB 10.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 266kB 10.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 276kB 10.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 286kB 10.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from stanza) (3.12.4)\n",
            "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.6/dist-packages (from stanza) (1.7.0+cu101)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from stanza) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from stanza) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from stanza) (1.19.5)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.6/dist-packages (from protobuf->stanza) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->stanza) (51.3.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.3.0->stanza) (3.7.4.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.3.0->stanza) (0.16.0)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.3.0->stanza) (0.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->stanza) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->stanza) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->stanza) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->stanza) (2020.12.5)\n",
            "Installing collected packages: stanza\n",
            "Successfully installed stanza-1.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-4u1y9ivT6t",
        "outputId": "d04c0904-68d9-4157-ff98-99ba1b32a28d"
      },
      "source": [
        "import stanza\r\n",
        "\r\n",
        "# This downloads the English models for the neural pipeline\r\n",
        "stanza.download('en')     \r\n",
        "\r\n",
        "# This sets up a default neural pipeline in English\r\n",
        "nlp = stanza.Pipeline('en') \r\n",
        "\r\n",
        "# Process a sentence.\r\n",
        "doc = nlp(\" The Google company leads the development of the Android mobile operating system, the Google Chrome web browser, and Chrome OS, a lightweight operating system based on the Chrome browser. Google has moved increasingly into hardware; from 2010 to 2015, it partnered with major electronics manufacturers in the production of its Nexus devices, and it released multiple hardware products in October 2016, including the Google Pixel line of smartphones, Google Home smart speaker, Google Wifi mesh wireless router, and Google Daydream virtual reality headset. Google has also experimented with becoming an Internet carrier (Google Fiber, Google Fi, and Google Station).\")\r\n",
        "\r\n",
        "\r\n",
        "# Display the text and type of entities the model found\r\n",
        "print('\\n{:<12}  {:}\\n'.format('Entity', 'Type'))\r\n",
        "\r\n",
        "# For each entity...\r\n",
        "for entity in doc.entities:\r\n",
        "\r\n",
        "    # Print the text and its type.\r\n",
        "    print('{:<12}  {:}'.format(entity.text, entity.type))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/master/resources_1.2.0.json: 128kB [00:00, 29.8MB/s]                    \n",
            "2021-01-28 02:34:35 INFO: Downloading default packages for language: en (English)...\n",
            "2021-01-28 02:34:37 INFO: File exists: /root/stanza_resources/en/default.zip.\n",
            "2021-01-28 02:34:42 INFO: Finished downloading models and saved to /root/stanza_resources.\n",
            "2021-01-28 02:34:42 INFO: Loading these models for language: en (English):\n",
            "=========================\n",
            "| Processor | Package   |\n",
            "-------------------------\n",
            "| tokenize  | combined  |\n",
            "| pos       | combined  |\n",
            "| lemma     | combined  |\n",
            "| depparse  | combined  |\n",
            "| sentiment | sstplus   |\n",
            "| ner       | ontonotes |\n",
            "=========================\n",
            "\n",
            "2021-01-28 02:34:42 INFO: Use device: cpu\n",
            "2021-01-28 02:34:42 INFO: Loading: tokenize\n",
            "2021-01-28 02:34:42 INFO: Loading: pos\n",
            "2021-01-28 02:34:42 INFO: Loading: lemma\n",
            "2021-01-28 02:34:42 INFO: Loading: depparse\n",
            "2021-01-28 02:34:42 INFO: Loading: sentiment\n",
            "2021-01-28 02:34:43 INFO: Loading: ner\n",
            "2021-01-28 02:34:43 INFO: Done loading processors!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Entity        Type\n",
            "\n",
            "Google        ORG\n",
            "Android       PRODUCT\n",
            "Google Chrome  PRODUCT\n",
            "Chrome OS     PRODUCT\n",
            "Chrome        PRODUCT\n",
            "Google        ORG\n",
            "2010 to 2015  DATE\n",
            "Nexus         ORG\n",
            "October 2016  DATE\n",
            "Google Pixel  PRODUCT\n",
            "Google Home   PRODUCT\n",
            "Google Wifi   PRODUCT\n",
            "Google Daydream  PRODUCT\n",
            "Google        ORG\n",
            "Google Fiber  ORG\n",
            "Google Fi     PRODUCT\n",
            "Google Station  ORG\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}